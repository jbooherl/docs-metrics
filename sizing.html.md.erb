---
breadcrumb: App Metrics Documentation
title: Sizing PCF Metrics
owner: PCF Metrics
list_style_none: true
---

<strong><%= modified_date %></strong>

You can configure PCF Metrics depending on your deployment size. Use the information in this topic to
optimize PCF Metrics for high capacity, reduce resource usage for smaller deployment sizes and to scale your overall deployment.

If you are not familiar with the PCF Metrics components, review [PCF Metrics Product Architecture](./architecture.html) before reading this topic.

For how to configure resources for a running deployment, see the following procedures:

+ [Procedure for Scaling the Metrics Datastore](#metrics-procedures)
+ [Procedure for Scaling the Log Datastore](#log-procedures)
+ [Procedure for Scaling the Temporary Datastore](#temp-procedures)
+ [Procedure for Scaling the Ingestor, Logqueues, and Metrics API](#ingestor-procedures)

## <a id='configs-by-size'></a> Sizing by deployment size

Use the following tables as a guide for configuring resources for your deployment.

For more information about the Metric Store, see [Metric Store documentation](https://docs.pivotal.io/metric-store/installing.html#-metric-store-for-pcf-resources).

Currently, you can scale vertically rather than horizontally.

For more information about limitations, see [Limitations](https://docs.pivotal.io/metric-store/index.html#limitations)
in the Metric Store documentation.

## <a id='configs-by-size'></a> Sizing by deployment size

Use the following tables as a guide to configure your resources for deployment.

Estimate the size of your deployment according to how many apps you expect to deploy.

<table style='nice'>
   <tr><th>Size</th><th>Purpose</th><th>Approximate number of app instances</th></tr>
   <tr><td><a href="#small">Small</a></td><td>Test use</td><td>100</td></tr>
   <tr><td><a href="#medium">Medium</a></td><td>Production use</td><td>5,000</td></tr>
   <tr><td><a href="#large">Large</a></td><td>Production use</td><td>15,000</td></tr>
</table>

If you are using Metrics Forwarder and custom metrics, you might need to scale up the MySQL Server instance
more than indicated in the tables below.
Pivotal recommends you start with the one of the following configurations and scale up as necessary
by following the steps in <a href="#metrics-datastore">Configuring the Metrics Datastore</a>.

###<a id='small'></a>Deployment resources for a small deployment

Example resource configuration to store approximately 14 days of data for a small deployment, about 100 application instances:

<table>
  <tr>
    <th>Job</th>
    <th>Instances</th>
    <th>Persistent Disk Type</th>
    <th width=40%>VM Type</th>
  </tr>
  <tr>
    <td>PostgreSQL Data</td>
    <td>1 (not configurable)</td>
    <td>1 TB</td>
    <td>xlarge (cpu: 4, ram: 16 GB, disk: 8 GB)</td>
  </tr>
  <tr>
    <td>Redis</td>
    <td>1 (not configurable)</td>
    <td>5 GB</td>
    <td>micro (cpu: 1, ram: 4 GB, disk: 8 GB)</td>
  </tr>
  <tr>
    <td>MySQL Server</td>
    <td>1 (not configurable)</td>
    <td>500 GB</td>
    <td>small (cpu: 1, ram: 4 GB, disk: 8 GB)</td>
  </tr>
</table>

###<a id='medium'></a>Deployment resources for a medium deployment

Example resource configuration to store approximately 14 days of data for a medium deployment, about 5000 application instances:

<table>
  <tr>
    <th>Job</th>
    <th>Instances</th>
    <th>Persistent Disk Type</th>
    <th width=40%>VM Type</th>
  </tr>
  <tr>
    <td>PostgreSQL Data</td>
    <td>1 (not configurable)</td>
    <td>12 TB</td>
    <td>2xlarge (cpu: 8, ram: 32 GB, disk: 16 GB)</td>
  </tr>
  <tr>
    <td>Redis</td>
    <td>1 (not configurable)</td>
    <td>20 GB</td>
    <td>medium (cpu: 4, ram: 16 GB, disk: 8 GB)</td>
  </tr>
  <tr>
    <td>MySQL Server</td>
    <td>1 (not configurable)</td>
    <td>2 TB</td>
    <td>medium (cpu: 4, ram: 16 GB, disk: 8 GB)</td>
  </tr>
</table>

###<a id='large'></a>Deployment resources for a large deployment

Example resource configuration to store approximately 14 days of data for a large deployment, about 15,000 application instances:

<table>
  <tr>
    <th>Job</th>
    <th>Instances</th>
    <th>Persistent Disk Type</th>
    <th width=40%>VM Type</th>
  </tr>
  <tr>
    <td>PostgreSQL Data</td>
    <td>1 (not configurable)</td>
    <td>32 TB</td>
    <td>4xlarge (cpu: 16, ram: 64 GB, disk: 32 GB)</td>
  </tr>
  <tr>
    <td>Redis</td>
    <td>1 (not configurable)</td>
    <td>60 GB</td>
    <td>2xlarge (cpu: 8, ram: 64 GB, disk: 16 GB)</td>
  </tr>
  <tr>
    <td>MySQL Server</td>
    <td>1 (not configurable)</td>
    <td>4 TB</td>
    <td>2xlarge (cpu: 8, ram: 32 GB, disk: 16 GB)</td>
  </tr>
</table>



##<a id='metrics-datastore'></a> Scaling metrics datastore


PCF Metrics stores metrics in a single MySQL node.
For PCF deployments with high app logs load, you can add memory and persistent disk to the MySQL server node.

###<a id='metrics-considerations'></a> Scaling metrics datastore considerations

While the default configurations in [Suggested Sizing by Deployment Size](#configs-by-size) above are a good starting point for your MySQL server node,
they do not take into account the additional load from custom metrics.
Pivotal recommends evaluating performance over a period of time and scaling upwards as necessary.
As long as persistent disk is scaled up, you won't not lose any data from scaling.
Please note PCF Metrics adds a day to your configured <code>Metrics Retention Window</code> to prevent pruning within your desired retention window.
The retention window begins at UTCÂ±00:00 of the current day and goes back the amount of days you enter in this field, plus the one more day added by PCF Metrics.


###<a id='metrics-procedures'></a> Scaling MySQL server node

Do the following to scale up the MySQL server node:

To scale up the MySQL server node, do the following:

1. Determine how much memory and persistent disk are required for the MySQL server node.
1. Go to Ops Manager Installation Dashboard and click the **Metrics** tile.
1. From the **Settings** tab of the **Metrics** tile, click **Resource Config**.
1. Select the values for the **Persistent Disk Type** and **VM Type**.
1. Click **Save**.

<p class="note warning"><strong>WARNING!</strong> If you are using PCF v1.9.x and earlier,
   there might be issues Ops Manager BOSH Director using persistent disks larger than 2&nbsp;TB.</p>

## <a id='log-datastore'></a> Scaling the Log Datastore

PCF Metrics uses Postgres to store logs.

### <a id='log-considerations'></a> Scaling considerations

Pivotal suggests estimating your Logs data storage needs using the equation below before configuring your Postgres instance.

The following calculation attempts to measure Postgres resource requirements more precisely depending on your logs load. This formula is only an approximation, and Pivotal suggests rounding the numbers up as a safety measure against undersizing Postgres:

1. Determine how many logs the apps in your deployment emit per hour (_R_) and the average size of each log (_S_).

2. Calculate the number of instances (_N_) and the persistent disk size for the instances (_D_) you need to scale to
   using the following formula:<br><br>

    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; _R_ &times; _S_ &times; 336 &times; 2 =  _N_ &times; _D_ <br><br>

    The formula assumes that a log retention period is 336 hours (2 weeks), and the [number of Elasticsearch replica shards](https://www.elastic.co/guide/en/elasticsearch/guide/current/replica-shards.html) is 1 (default).
    <br><br>
    For example:

    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 200,000 logs/hr &times; 25 KB &times; 336 hr &times; 2 &asymp; 7 instances &times; 500 GB<br>
    or<br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 200,000 logs/hr &times; 25 KB &times; 336 hr &times; 2 &asymp; 3 instances &times; 1 TB<br>

   As previously stated, the default Elasticsearch configuration sets the number of replicas to 1, which means that every shard has 1 replica and Elasticsearch logs are HA by default.

### <a id='log-procedures'></a> Scaling

<p class="note warning">
<strong>WARNING!</strong> When you vertically scale your Postgres instance, Postgres enters an unhealthy period during which it does not ingest any new logs data until the scaling operation has completed.</p>

After determining the desired size for the Postgres instance needed for your deployment, perform the following steps to scale your nodes:

1. Go to the Ops Manager Installation Dashboard and click the **Metrics** tile.
1. From the **Settings** tab of the **Metrics** tile, click **Resource Config**.
1. Locate the **Elasticsearch Data** job and select the drop-down menu under **Instances** to change the number of instances.
1. Click **Save**.

## <a id='temp-datastore'></a> Scaling the temporary Redis datastore

PCF Metrics uses Redis to temporarily store ingested data from the Loggregator Firehose
as well as cache data queried by the Metrics API.
The former use case is to prevent major metrics and logs loss when the data stores (Postgres and MySQL) are unavailable.
The latter is to potentially speed up front-end queries. See [PCF Metrics Product Architecture](./architecture.html) for more information.

### <a id='temp-considerations'></a> Scaling considerations

The default Redis configuration specified in [Suggested Sizing by Deployment Size](#configs-by-size) above
that fits your deployment size should work for most cases.
Redis stores all data in memory, so if your deployment size requires it,
you can also consider scaling up the RAM for your Redis instance.

### <a id='temp-procedures'></a> Scaling the size of Redis VM

Follow these steps to configure the size of the Redis VM for the temporary datastore based on your calculations.

<p class="note"><strong>Note</strong>: In the case that the temporary datastore becomes full,
Redis uses the <code>volatile-ttl</code> eviction policy to continue storing incoming logs.
For more information, see <i>Eviction policies</i> in <a href="https://redis.io/topics/lru-cache">Using Redis as an LRU cache</a>.</p>

1. Go to Ops Manager Installation Dashboard and click the **Metrics** tile.
1. From the **Settings** tab, click **Resource Config**.
1. Locate the **Redis** job and select the dropdown menus under **Persistent Disk Type** and **VM Type** to scale Redis up or down.
1. Click **Save**.

## <a id='ingestor'></a> Scaling the Ingestor, Logqueues, and Metrics API

The procedures for scaling the Metrics Ingestor, Logs Queue, Metrics Queue, and Metrics API instances are similar.

+ **Metrics Ingestor** â PCF Metrics deploys the Ingestor as an app, `metrics-ingestor`, within PCF.
The Ingestor consumes logs and metrics from the Loggregator Firehose at the rate of _loggregator.doppler.ingress_, sending metrics and logs to their respective Logqueue apps.

    To customize PCF Metrics for high capacity, you can scale the number of Ingestor app instances and increase the amount of memory per instance.

+ **Logs Queue** â PCF Metrics deploys a **Metrics Queue** and an **Logs Queue** as apps,
    `metrics-queue` and `logs-queue`, within PCF.
  The Metrics Queue consumes metrics from the Ingestor and forwards them to MySQL. The Logs Queue consumes logs from the Ingestor and forwards them to Postgres.

    To customize PCF Metrics for high capacity, you can scale the number of queue app instances and increase the amount of memory per instance.

    The number of Metrics and Logs Queues needed is dependent on the frequency that logs and metrics are forwarded by the Ingestor.
    As a general rule:
    +  For every 45,000 logs per minute, add 2 Logs Queues.
    +  For every 17,000 metrics per minute, add 1 Metrics Queue.

    The above is a general estimate. You might need fewer instances depending on your deployment.
    To optimize resource allocation, provision fewer instances initially and increase instances until you achieve desired performance.

+ **Metrics Alerting API** â PCF Metrics deploys the app, `metrics-alerting`, within PCF. The Metrics Alerting app is reposnsible for creating notifications for the user-created metrics and events monitors.<br><br>
Please note that PCF Metrics 1.6 only supports one instance of this API per installation.

+ **Metrics API** â PCF Metrics deploys the app, `metrics`, within PCF.

Refer to this table to determine how many instances you need for each component.


| Item | Small | Medium | Large |
|----------------------------------------|--------------------------------|-------------------------------|--------------------------------|
| Ingestor instance count                | Number of Doppler servers      | Number of Doppler servers     | Number of Doppler servers      |
| Metrics Queue instance count           | 1                              | 2                             | 8                              |
| Logs Queue instance count              | 1                              | 4                             | 11                             |
| Metrics API instance count             | 1                              | 2                             | 6                              |
| Metrics Alerting API instance count    | 1                              | 1                             | 1                              |

Find the number of Doppler servers in the Resource Config pane of the Pivotal Application Service tile.

### <a id='ingestor-considerations'></a> Scaling considerations

Pivotal recommends starting with the configuration in [Suggested Sizing by Deployment Size](#configs-by-size), for your deployment size
and then evaluating performance over a period of time and scaling upwards if performance degrades.

### <a id='ingestor-procedures'></a> Scaling the apps

<p class="note warning"><strong>WARNING! </strong> If you decrease the number of instances,
   you might lose data currently being processed on the instances you eliminate.</p>

After determining the number of instances needed for your deployment,
perform the following steps to scale:

1. Target your Cloud Controller with the Cloud Foundry Command Line Interface (cf CLI).
   If you have not installed the cf CLI, see [Installing the cf CLI](http://docs.pivotal.io/pivotalcf/cf-cli/install-go-cli.html).
	<pre class="terminal">
	$ cf api api.YOUR-SYSTEM-DOMAIN
	Setting api endpoint to api.YOUR-SYSTEM-DOMAIN...
	OK
	API endpoint:   <span>https:</span>//api.YOUR-SYSTEM-DOMAIN (API version: 2.54.0)
	Not logged in. Use 'cf login' to log in.
	</pre>

2. Log in with your UAA administrator credentials.
   To retrieve these credentials, go to the **Pivotal Elastic Runtime** tile in the Ops Manager Installation Dashboard and click **Credentials**.
   Under **UAA**, click **Link to Credential** next to **Admin Credentials** and record the password.
	<pre class="terminal">
	$ cf login
	API endpoint: <span>https:</span>//api.YOUR-SYSTEM-DOMAIN

	Email> admin
	Password>
	Authenticating...
	OK

3. When prompted, target the `metrics-v1-6` space.
	<pre class="terminal">
	Targeted org system

	Select a space (or press enter to skip):
	<span>1</span>. system
	<span>2</span>. notifications-with-ui
	<span>3</span>. autoscaling
	<span>4</span>. metrics-v1-6

	Space> 4
	Targeted space metrics-v1-6

	API endpoint:   <span>https:</span>//api.YOUR-SYSTEM-DOMAIN (API version: 2.54.0)
	User:           admin
	Org:            system
	Space:          metrics-v1-6
	</pre>

4. List the apps that are running in the `metrics-v1-6` space.
	<pre class="terminal">
$ cf apps
Getting apps in org system / space metrics-v1-6 as admin...
OK<br>
name                    requested state   instances   memory   disk   urls
metrics-queue-blue      stopped           0/1         512M     1G
metrics-blue            stopped           0/1         1G       2G
metrics-ui-blue         stopped           0/1         256M     1G
metrics-alerting-blue   stopped           0/1         1G       2G
metrics-ingestor-blue   stopped           0/2         384M     1G
logs-queue-blue         stopped           0/1         256M     1G
metrics-ingestor        started           2/2         384M     1G
metrics-queue           started           1/1         512M     1G
logs-queue              started           1/1         256M     1G
metrics-ui              started           1/1         256M     1G     metrics.YOUR-SYSTEM-DOMAIN
metrics-alerting        started           1/1         1G       2G
metrics                 started           1/1         1G       2G     metrics.YOUR-SYSTEM-DOMAIN/api/v1
</pre>


5. Scale the app to the desired number of instances:

    <code>cf scale APP-NAME -i INSTANCE-NUMBER</code>

    Where the `APP-NAME` is `logs-queue`, `metrics`, `metrics-ingestor`, or `metrics-queue`.<br>
    For example, to scale all the apps:
	<pre class="terminal">$ cf scale logs-queue -i 2
    $ cf scale metrics -i 2
    $ cf scale metrics-ingestor -i 2
    $ cf scale metrics-queue -i 2</pre>

1. Evaluate the CPU and memory load on the instances:

    <code>cf app APP-NAME</code>

    For example,
	<pre class="terminal">
	$ cf app metrics-ingestor
	Showing health and status for app metrics-ingestor in org system / space metrics as admin...
	OK
	<br>
	requested state: started
	instances: 1/1
	usage: 1G x 1 instances
	urls:
	last uploaded: Sat Apr 23 16:11:29 UTC 2016
	stack: cflinuxfs2
	buildpack: binary_buildpack
	<br>
	     state     since                    cpu    memory        disk           details
	<span>#</span>0   running   2016-07-21 03:49:58 PM   2.9%   13.5M of 1G   12.9M of 1G
	</pre>

1. If your average memory usage exceeds 50% or your CPU consistently averages over 85%,
   add more instances with `cf scale APP-NAME -i INSTANCE-NUMBER`.
   <br><br>
   In general, you should scale the app by adding additional instances.
   However, you can also scale the app by increasing the amount of memory per instance:

        cf scale APP-NAME -m NEW-MEMORY-LIMIT

    For example,
	<pre class="terminal">$ cf scale metrics-ingestor -m 2G</pre>

	For more information about scaling app instances, see [Scaling an Application Using cf scale](http://docs.pivotal.io/pivotalcf/devguide/deploy-apps/cf-scale.html).
Maximum ingress throughput is primarily dependent on the number of VMs,
with secondary consideration to CPU and Memory resources.
Retention and Replication Factor primarily depends on the size of persistent disks attached to the VMs.
Abnormally high cardinality of indexed information (particularly source_id and instance_id) can place pressure
on VM Memory.
This is possible even in the absence of high log volume.
