---
breadcrumb: App Metrics Documentation
title: Troubleshoot App Metrics
owner: App Metrics
list_style_none: true
---

<strong><%= modified_date %></strong>
You can use the following techniques to troubleshoot integration issues that arise in App Metrics.

## <a id="integration-errors"></a> Integration errors

App Metrics is a nexus of four integration points:

* CF API
* UAA
* Metric Store
* Log Store

To monitor the status of these dependencies, check out the `/integration-status` endpoint.
It's a quick way to see if the app is able to communicate with each dependency:

* `true` - accessible

* `false` - not accessible

For example, when all integrations are accessible the output is:

    {"Log Store":true,"CF API":true,"UAA":true,"Metric Store":true}

If there is an issue with one of the dependencies, investigate what is wrong with that particular component.

1. Use `bosh ssh` to access the VM from the previous step.
For instructions, see [Advanced Troubleshooting with the BOSH CLI](https://docs.vmware.com/en/VMware-Tanzu-Operations-Manager/3.0/vmware-tanzu-ops-manager/install-trouble-advanced.html).

1. Run the following command to list all the Elasticsearch indices:

  <pre class="terminal">$ curl ELASTICSEARCH-HOST-IP:9200/\_cat/indices?v | sort
  <br>
  green  open   app\_logs\_1477512000   8   1  125459066            0     59.6gb         29.8gb
  green  open   app\_logs\_1477526400   8   1  129356671            0     59.1gb         29.5gb
  green  open   app\_logs\_1478174400   8   1  129747170            0     61.9gb         30.9gb
  . . .
  green  open   app\_logs\_1478707200   8   1  128392686            0     63.2gb         31.6gb
  green  open   app\_logs\_1478721600   8   1  102005754            0     53.5gb         26.5gb
  health status index               pri rep docs.count docs.deleted store.size pri.store.size
  </pre>

3. If the `curl` command does not return a `success` response, Elasticsearch might not even be running correctly. Inspect the following logs for any failures or errors:
      * `/var/vcap/sys/log/elasticsearch/elasticsearch.stdout.log`
      * `/var/vcap/sys/log/elasticsearch/elasticsearch.stderr.log`

1. Examine the `status` column of the output.
1. If any of the indices are `red`, delete them using the following command:

    <pre class="terminal">curl -X DELETE ELASTICSEARCH-HOST-IP:9200/INDEX</pre>

3. Start the `elasticsearch-logqueue` app again:

    <pre class="terminal">$ cf restart elasticsearch-logqueue</pre>

  4. From each of the Elasticsearch VMs, run the following command:

      <pre class="terminal">$ monit restart all</pre>

  5. Check periodically to verify the indices gradually recover to a `green` status.

  6. Run the `curl` command several more times and examine the most recent index to see if the number of stored documents periodically increases.

     The last row of the output corresponds to the most recent index.
     The sixth column displays the number of documents for the index


  7. If all indices show a `green` status, but the number of documents does not increase,
     there is likely a problem further up in ingestion.
     Proceed to to [Checking the Elasticsearch Logqueue](#check-es-logqueue).

4. Check whether cluster-level shard allocation is enabled:

    <pre class="terminal">$ curl localhost:9200/_cluster/settings</pre>

    Examine the output:

   `"all"` means the shard allocation is enabled.

   `"none"` means the shard allocation is deactivated.

1. Enable the shard allocation again by running the following command:
    <pre class="terminal">$ curl -XPUT localhost:9200/_cluster/settings -d
    {"transient" : {
      "cluster.routing.allocation.enable" : "all"
    }}</pre>

2. Check whether a proxy is present in front of Elasticsearch and whether HTTP traffic is enabled:
3. Use `cf ssh` to SSH into any app in the metrics space:

    <pre class="terminal">$ cf ssh APP-NAME</pre>

  1. Run the `curl` command with the IP address of the **Elasticsearch Master** node:

      <pre class="terminal">$ curl ELASTICSEARCH-MASTER-IP-ADDRESS</pre>

  1. If the `curl` command fails, talk to your system administrator about removing the proxy or green-listing the App Metrics apps.

### <a name="check-es-logqueue"></a>Checking the Elasticsearch Logqueue

1. Run <code>cf apps</code> to see if the <code>elasticsearch-logqueue</code> app instances are `started`.

1. If any instance of the app is `stopped`, run the following command to increase logging:

    <pre class="terminal">$ cf set-env elasticsearch-logqueue LOG_LEVEL DEBUG</pre>

  1. Run the following command to stream logs:

      <pre class="terminal">$ cf logs elasticsearch-logqueue</pre>

  1. In a different terminal window, run the following command:

      <pre class="terminal">$ cf restage elasticsearch-logqueue</pre>

  1. Watch the logs emitted by the `elasticsearch-logqueue` app for errors.
      * A common error is that the app cannot connect to Elasticsearch
        because a user deleted the application security group (ASG)
        that App Metrics creates to allow the Logqueue app to connect to the Elasticsearch VMs.
        You can run `cf security-group metrics-api` to see if the ASG exists.
        If not, see [App Security Groups](https://docs.cloudfoundry.org/concepts/asg.html).

  1. If the app is started and you do not find any errors, proceed to [Checking the Metrics Ingestor](#check-ingestor).

### <a name="check-ingestor"></a>Checking the Metrics Ingestor

1. Run <code>cf apps</code> to see if the `metrics-ingestor` app instances are `started`.
1. If any of the app instances are `stopped`, run the following command to increase logging:

    <pre class="terminal">$ cf set-env metrics-ingestor LOG_LEVEL DEBUG</pre>

  1. Run the following command to stream logs:

      <pre class="terminal">$ cf logs metrics-ingestor</pre>

  1. In a different terminal window, run the following command:

      <pre class="terminal">$ cf restage metrics-ingestor</pre>

  1. Watch the logs emitted by the `metrics-ingestor` app for errors. See the following list for common errors:
      * **Cannot connect to the firehose**: App Metrics creates a UAA user to authenticate the connection to the Firehose.
          This user must have the `doppler.firehose` authority.
      * **Cannot connect to the logqueues**: There might be a problem with the UAA, or it could be throttling traffic.
      * **WebSocket Disconnects**: If you see WebSocket disconnects logs in the Ingestor app, consider adding additional Ingestor instances.
          The Firehose might be dropping the Ingestor connection to avoid back pressure.

  1. If the app is started and you do not find any errors, proceed to [Checking MySQL](#check-mysql).

### <a name="check-mysql"></a>Checking MySQL

1. From Tanzu Operations Manager, select the App Metrics tile.

1. Under the **Status** tab, record the IP of a **MySQL Server** node.

1. Use `bosh ssh` to access the VM from the previous step. For instructions, see [Advanced Troubleshooting with the BOSH CLI](https://docs.vmware.com/en/VMware-Tanzu-Operations-Manager/3.0/vmware-tanzu-ops-manager/install-trouble-advanced.html).

2. Log in to mysql by running `mysql -u USERNAME -p PASSWORD`

  <p> If you do not know the username and password,
    you can run `cf env mysql-logqueue` with the <code>system</code> org and the <code>metrics-v1-4</code> space targeted.</p>

1. Verify that the database was bootstrapped correctly:
1. Run `show databases` and check for a `metrics` database.
      1. If there is no `metrics` database, the `migrate_db` errand of the BOSH release might not have run or succeeded.
         Ensure the errand is selected in the tile configuration and update the tile.

1. Run `use metrics` to select the `metrics` database:
   <pre class="terminal">mysql> use metrics;</pre>

1. Run `show tables` and ensure you see the following tables:
   <pre class="terminal">mysql> show tables;
   +-------------------+
   | Tables\_in\_metrics |
   +-------------------+
   | app\_event         |
   | app\_metric        |
   | app\_metric\_rollup |
   | schema\_version    |
   +-------------------+
   </pre>

1. Enter the following query several times to verify that the value returned does not decrease over time:

    <pre class="terminal">mysql> select count(*) from metrics.app\_metric\_rollup where timestamp_minute > ((UNIX\_TIMESTAMP() - 60)* POW(10, 3));</pre>
  This command displays the rate at which metrics flow in over the last minute.
      1. If the command returns `0` or a consistently decreasing value, the problem is likely further up in ingestion.
         Proceed to [Checking the MySQL Logqueue](#check-mysql-logqueue).

### <a name="check-mysql-logqueue"></a> Checking the MySQL Logqueue

1. Run <code>cf apps</code> to see if the <code>mysql-logqueue</code> app instances are `started`.

1. If any instance of the app is `stopped`, run the following command to increase logging:

    <pre class="terminal">$ cf set-env mysql-logqueue LOG_LEVEL DEBUG</pre>

  1. Run the following command to stream logs:

      <pre class="terminal">$ cf logs mysql-logqueue</pre>

  1. In a different terminal window, run the following command:

      <pre class="terminal">$ cf restage mysql-logqueue</pre>

  1. Watch the logs emitted by the `mysql-logqueue` app for errors.
      * A common error is that the app cannot connect to MySQL
        because a user deleted the application security group (ASG)
        that App Metrics creates to allow the Logqueue app to connect to the MySQL VMs.
        You can run `cf security-group metrics-api` to see if the ASG exists.
        If not, see [Creating Application Security Groups](https://docs.vmware.com/en/VMware-Tanzu-Application-Service/4.0/tas-for-vms/asg.html).

## <a id='mysql'></a> MySQL Node failure

In some cases, a MySQL server node might fail to restart.
The following two sections describe the known conditions that cause this failure as well as steps for diagnosing and resolving them.
If neither of the causes listed apply, the final section provides instructions for re-deploying BOSH as a last resort to resolve the issue.

### Cause 1: Monit Timed Out

#### Diagnose

Follow these steps to see if a `monit` time-out caused the MySQL node restart to fail:

1. Use `bosh ssh` to access the failing node, using the IP address in the Ops Manager Director tile **Status** tab.
   For instructions, see [Advanced Troubleshooting with the BOSH CLI](https://docs.vmware.com/en/VMware-Tanzu-Operations-Manager/3.0/vmware-tanzu-ops-manager/install-trouble-advanced.html).
1. Run `monit summary` and check the status of the `mariadb_ctrl` job.
1. If the status of the `mariadb_ctrl` job is `Execution Failed`, open the following file: `/var/vcap/sys/log/mysql/mariadb_ctrl.combined.log`.
1. If the last line of the log indicates that MySQL started without issue, such as in the following example,
     `monit` likely timed out while waiting for the job to report healthy. Use the following the steps to resolve the issue.

  <pre class="terminal">
  {"timestamp":"1481149250.288255692","source":"/var/vcap/packages/<br>mariadb\_ctrl/bin/mariadb\_ctrl","message":"/var/vcap/packages/<br>mariadb\_ctrl/bin/mariadb\_ctrl.mariadb\_ctrl<br> started","log\_level":1,"data":{}}
  </pre>

#### Resolve

Run the following commands to return the `mariadb_ctrl` job to a healthy state:

1. Run `monit unmonitor mariadb`.
1. Run `monit monitor mariadb`.
3. Run `monit summary` and confirm that the output lists `mariadb_ctrl` as `running`.

### Cause 2: Bin Logs Filled up the Disk

#### Diagnose

1. Use `bosh ssh` to access the failing node.
   For instructions, see [Advanced Troubleshooting with the BOSH CLI](https://docs.vmware.com/en/VMware-Tanzu-Operations-Manager/3.0/vmware-tanzu-ops-manager/install-trouble-advanced.html).
1. Open the following log file: `/var/vcap/sys/log/mysql/mysql.err.log`.
1. If you see log messages that indicate insufficient disk space, the [persistent disk](https://bosh.io/docs/persistent-disks.html) is likely storing too many bin logs.
   Confirm insufficient disk space by doing the following:
1. Run `df -h`.
      1. Ensure that you see the `/var/vcap/store` folder is at or over `90%` usage.
1. Go to `/var/vcap/store/mysql` and run `ls -al`.
      1. Ensure that you see many files named with the format `mysql-bin.########`.

In MySQL for PCF, the server node does not make use of these logs and you can remove all except the most recent bin log.
Follow the steps below to resolve the issue.

#### Resolve

1. Log in to mysql by running `mysql -u USERNAME -p PASSWORD`. If you do not know the username and password, you can run `cf env mysql-logqueue` with the <code>system</code> org
and the <code>metrics-v1-4</code> space targeted.
1. Run `use metrics;`.
2. Run the following command:
<pre class="terminal">mysql> PURGE BINARY LOGS BEFORE 'YYYY-MM-DD HH:MM:SS'; </pre>

### Redeploying BOSH to restart the node

If troubleshooting is based on the causes that were previously mentioned did not resolve the issue with your failing MySQL node,
you can follow the steps below to recover it.
VMware recommends only using this procedure as a if there are no other potential solutions available.

<p class="note important">
<span class="note__title">Important</span>
This procedure is extremely costly in terms of time and network resources.
The cluster takes a significant amount of time to put the data replicated to the rest of the cluster back into the rebuilt node.
This procedure consumes considerable network bandwidth as potentially hundreds of gigabytes of data needs to transfer.

#### Stopping the Ingestor App

1. From Tanzu Operations Manager, click the **Elastic Runtime Tile**.
1. Click the **Credentials** tab.
1. Under the **UAA** job, next to **Admin Credentials**, click **Link to Credential**.
1. Record the username and password for use in the next step.
1. Log in to the cf CLI using the credentials from the previous step.

    <pre class="terminal">$ cf login -a http<span>s</span>://api.YOUR-SYSTEM-DOMAIN -u admin -p PASSWORD</pre>

1. Target the <code>system</code> org and <code>metrics-v1-4</code> space of your PCF deployment:
    <pre class="terminal">$ cf target -o system -s metrics-v1-4</pre>
1. Stop data flow into the Galera cluster:
    <pre class="terminal">$ cf stop metrics-ingestor</pre>

#### Editing your Deployment Manifest

1. Follow the steps in [Log in to BOSH](https://docs.vmware.com/en/VMware-Tanzu-Operations-Manager/3.0/vmware-tanzu-ops-manager/install-trouble-advanced.html)
to target and log in to your BOSH Director.
   The steps vary slightly depending on whether your PCF deployment uses internal authentication or an external user store.
1. Download the manifest of your PCF deployment:
    <pre class="terminal">
    $ bosh download manifest YOUR-PCF-DEPLOYMENT YOUR-PCF-MANIFEST.yml
    </pre>
    You must know the name of your TAS for VMs deployment to download the manifest. To retrieve it, run <code>bosh deployments</code>
    to list your deployments and locate the name of your TAS for VMs deployment.
1. Open the manifest and set the number of instances of the failed server node to `0`.
1. Run `bosh deployment YOUR-PCF-MANIFEST.yml` to specify your edited manifest.
1. Run `bosh deploy` to deploy with your manifest.
1. Run `bosh disks --orphaned` to see the [persistent disk](https://bosh.io/docs/persistent-disks.html) or disks associated with the failed node.
1. Record the `CID` of each persistent disk.
1. Contact [VMware Support](https://www.vmware.com/support/services.html) to walk through re-attaching the orphaned disks to new VMs to preserve their data.
1. Open the manifest and set the number of instances of the failed server node to `1`.
1. Run `bosh deploy` to deploy with your edited manifest.
1. Wait for BOSH to rebuild the node.

## <a id='log'></a> Log errors

<table class=“table”>
<thead>
<tr>
  <th width="22%">Error</th>
  <td>
    The PCF Metrics UI does not show any new logs from Elasticsearch.
  </td>
</tr>
</thead>
<tr>
  <th>Cause</th>
  <td>The tile deployed with the <code>Push App Metrics Data Components</code> errand deselected</td>
</tr>
<tr>
  <th>Solution</th>
  <td>
  Restart the Elasticsearch Logqueue using the cf CLI as follows:
  <ol>
    <li>Target the <code>system</code> org and <code>metrics-v1-4</code> space of your PCF deployment:</li>
    <pre class="terminal">$ cf target -o system -s metrics-v1-4</pre>
    <li>Run the following command to restart the Logqueue app:</li>
    <pre class="terminal">$ cf restart elasticsearch-logqueue</pre>
  </ol>
    <p class="note">
    <span class="note__title">Note</span>
   To avoid having to apply this fix in the future,
    select the check box to enable the <code>Push App Metrics Data Components</code> <a href="./installing.html#errands">errand</a> before your next tile update.

  </td>
</tr>
</table>

## <a id="503"></a>503 errors

<table class=“table”>
<thead>
  <tr>
      <th>Error</th>
      <td>You encounter <code>sorry, too many clients already</code> errors when accessing the App Metrics UI in your browser.</td>
  </tr>
  </thead>
  <tr>
      <th>Cause</th>
      <td>Your PostgreSQL is running out of disk space, causing reduced performance and a spike of open connections.
          Possible causes:
      <br><br>
      <ol>
        <li>If you are on <code>1.6.0</code>, your PostgreSQL instance does not have automatic pruning configured. This was introduced in <code>1.6.1</code></li>
        <li>Your app emits a lot of logs and is taking up disk space very quickly.</li>
      </ol>
    </td>
  </tr>
  <tr>
      <th>Solution</th>
      <td>
        <ol>
          <li>Upgrade your version to <code>1.6.1</code> and configure your <b>Logs Max Retention Percentage</b> and
            <b>Logs Disk Size Pruning Interval</b>.
          <li>
            Decrease the Logs Retention Window and increase the PostgreSQL Persistent Disk
            <br><br>
            <ol>
              <li>Go to the <b>Metrics Components Config</b> section of the App Metrics Tile.</li>
              <li>Decrease the Logs Retention Window to a smaller number to free up space in PostgreSQL.</li>
              <li>Go to the <b>Resource Config</b> section of the App Metrics Tile.</li>
              <li>Increase the Persistent Disk size of PostgreSQL Server to at least twice the current size.</li>
            </ol>
          </li>
        </ol>

  </tr>
</table>

## <a id="fetch-apps"></a>Failed to fetch apps
<table class=“table”>
<thead>
  <tr>
      <th>Error</th>
      <td>Even though you entered the correct UAA credentials, the metrics app fails to fetch the list of apps.</td>
  </tr>
  </thead>
  <tr>
      <th>Cause</th>
      <td>The browser plug-ins or cookies inject extraneous content in requests to Cloud Controller API, causing it to reject the request.</td>
  </tr>
  <tr>
      <th>Solution</th>
      <td>Confirm the problem and clear the browser, as follows:
      <ol>
        <li>Try the browser's incognito mode to see if the metrics app is able to fetch the list of apps.
             If this works, the problem is likely cookies or plugins.</li>
        <li>Clear your browser cookies and plugins.</li>
       </ol>
       </td>
  </tr>
</table>

## <a id="redis-full"></a>Redis temporary datastore stops accepting metrics

<table class=“table”>
<thead>
  <tr>
  </thead>
      <th>Error</th>
      <td>
            You see both these problems:<ul>
            <li>Metrics stop appearing on the UI.</li>
            <li>When you run <code>cf metrics-ingestor logs</code>, you see the following entry in the Ingestor logs:<br>
                <code>MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk.
                Commands that may modify the data set are disabled. Please check Redis logs for details about the error.</code></li>
           </ul>
      </td>
  </tr>
  <tr>
      <th>Cause</th>
      <td>The Redis datastore is full. The component is out of memory or persistent disk space.</td>
  </tr>
  <tr>
      <th>Solution</th>
      <td>Confirm the problem and scale up Redis, as follows:
      <ol>
        <li>On the <strong>Metrics</strong> tile, click the <strong>Status</strong> tab and
            look to see if the memory or persistent disk usage of the Redis job is over 80%.</li>
        <li>Scale up the Redis component. For more information, see <a href=".//sizing.html#temp-datastore">
            Scale the Temporary Datastore (Redis)</a>.</li>
       </ol>
       </td>
  </tr>
</table>

## <a id="fetch-apps"></a>Failed to fetch logs or logs were missing

<table class=“table”>
<thead>
  <tr>
      <th>Error</th>
        <td>In some cases, one or more noisy apps created an enormous amount of logs per second for the log store.
        </td>
  </tr>
  </thead>
      <th>Cause</th>
      <td>
      Here are some possible causes:
      <ul>
      <li> A shortening of the stored data retention period. Most of the disk space was occupied by one app log.</li>
      <li> A drop of input log envelopes occurs in the early ingestion stage.</li>
      <li> A load imbalance causes a non distributable load.</li>
      <li> A situation occurs when neither vertical or horizontal scaling was helpful.</li>
      <li> A large log size occurs and the querying became challenging for a particular sourceID.</li>
      <li> Log Store has an out of memory issue.</li>
  </li>
      </ul>
  <tr>
      <th>Solution</th>
      <td>Identify the Application Log Rate Limit definition and noisy log producer identification. Cloud Foundry supports log rate
      limiting to avoid noisy logging and easy identification of limit exceeding.
      See the following links for more details on how to
      define log rate limits for apps in Cloud Foundry and the methods that identify limit exceeding cases:
      <ul>
      <li><a href="https://docs.vmware.com/en/VMware-Tanzu-Application-Service/4.0/tas-for-vms/app-log-rate-limits.html">App Rate limiting</a></li>
      <li>
     <a href="https://docs.cloudfoundry.org/devguide/deploy-apps/manifest-attributes.html#log-rate-limit">Cloud Foundry manifest attribute</a></li>
      </ul>
      </li>
      </td>
  </tr>
</table>
